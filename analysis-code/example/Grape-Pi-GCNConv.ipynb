{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:09:29.940517Z",
     "start_time": "2023-10-13T18:09:18.565364Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from graphgym.custom_graphgym.loader.protein import ProteinDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "os.chdir('/Users/cgu3/Documents/Grape-Pi')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:09:29.961531Z",
     "start_time": "2023-10-13T18:09:29.946045Z"
    }
   },
   "id": "7dd95a8f8f46ccce"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rebuilding...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = ProteinDataset(\"data/single\", numeric_columns=['protein_probability'], label_column='protein_probability_soft_label', rebuild=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:09:40.017408Z",
     "start_time": "2023-10-13T18:09:29.953631Z"
    }
   },
   "id": "f563e02655d85d5c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import copy\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = dataset[0].to(device, 'x', 'y', 'train_mask', 'val_mask', 'test_mask')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:10:18.765121Z",
     "start_time": "2023-10-13T18:10:18.748746Z"
    }
   },
   "id": "93a70c5ac9301f0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a08a8bc020345fc0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1, 16)\n",
      "  (lin1): GCNConv(16, 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.nn as nn\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.lin1 = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.lin1(x)  \n",
    "        return x\n",
    "\n",
    "model = GCN(num_node_features=dataset.num_node_features, hidden_channels=16, num_classes=2)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T18:12:00.692008Z",
     "start_time": "2023-10-13T18:12:00.661764Z"
    }
   },
   "id": "d83b2a1abc6bf813"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import Javascript  # Restrict height of output cell.\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GCN(num_node_features=dataset.num_node_features, hidden_channels=10, num_classes=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(dataset.x, dataset.edge_index)  # Perform a single forward pass.\n",
    "      train_loss = criterion(out[dataset.train_mask], dataset.y[dataset.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      train_correct = pred[dataset.train_mask] == dataset.y[dataset.train_mask]  # Check against ground-truth labels.\n",
    "      train_acc = int(train_correct.sum()) / int(dataset.train_mask.sum())\n",
    "      train_loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return float(train_loss.detach()), train_acc\n",
    "\n",
    "\n",
    "def val():\n",
    "      model.eval()\n",
    "      out = model(dataset.x, dataset.edge_index)\n",
    "      val_loss = criterion(out[dataset.val_mask], dataset.y[dataset.val_mask])\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      val_correct = pred[dataset.val_mask] == dataset.y[dataset.val_mask] # Check against ground-truth labels.\n",
    "      val_acc = int(val_correct.sum()) / int(dataset.val_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return float(val_loss.detach()), val_acc\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(dataset.x, dataset.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[dataset.test_mask] == dataset.y[dataset.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(dataset.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc\n",
    "\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "val_loss_history = []\n",
    "for epoch in range(1, 101):\n",
    "    train_loss, train_acc = train()\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "    val_loss, val_acc = val()\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train loss: {train_loss:.4f} Train acc: {train_acc:.4f} '\n",
    "          f'Validation loss: {val_loss: .4f}, Validation acc: {val_acc:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27e7c05e73626eec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
